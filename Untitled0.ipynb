{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhidL6No/2oYiZmCemyqtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenokCodez/CSEC_DS/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdCGpFkS7wA0"
      },
      "outputs": [],
      "source": [
        "[1]\n",
        "\n",
        "list_example = [1, 2, 3, 4, 5]\n",
        "dict_example = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n",
        "import numpy as np\n",
        "array_example = np.array([1, 2, 3, 4, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A list is like a collection of items in order, and you can put different types of things in it, like numbers or strings. You access items by their position (index). A dictionary is different because it stores key-value pairs, so you look up values using keys, not numbers. It’s good for when you want to map things, like names to ages. A NumPy array is special for math and data stuff—it’s faster and works better with numbers, but it only holds one type of data, like all integers. In the examples, I showed a list with numbers, a dictionary with a person’s info, and a NumPy array with numbers too."
      ],
      "metadata": {
        "id": "eXX-GHuQ8xQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[2]\n",
        "\n",
        "def square_evens(numbers):\n",
        "    result = []\n",
        "    for num in numbers:\n",
        "        if num % 2 == 0:\n",
        "            result.append(num * num)\n",
        "    return result\n",
        "\n",
        "my_list = [10, 15, 20, 25, 30]\n",
        "squared_evens = square_evens(my_list)\n",
        "print(squared_evens)"
      ],
      "metadata": {
        "id": "1DhyhL5K87T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a list of numbers and checks each one to see if it’s even by using the modulo operator (%). If the number divided by 2 has no remainder (equals 0), it’s even, and then I square it by multiplying it by itself. I store all the squared even numbers in a new list and return it. For the list [10, 15, 20, 25, 30], it should only square 10, 20, and 30 because they’re even, so the output should be [100, 400, 900]. I printed it to see the result. It’s pretty straightforward, but I could maybe make it shorter with list comprehension if I knew how."
      ],
      "metadata": {
        "id": "OEHDKzGG9D60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[3]\n",
        "\n",
        "x = [1, 2, 3]\n",
        "y = x\n",
        "y.append(4)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "PuKyQKNz9IxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code outputs [1, 2, 3, 4]. Here’s why: when I set y = x, both variables point to the same list in memory, not a copy. So, when I append 4 to y, it changes the same list that x is pointing to. Lists in Python are mutable, which means they can change after you create them, and assigning one list to another variable doesn’t make a new list—it just makes another name for the same list. If I wanted y to be a separate copy, I should have used y = x.copy() or y = list(x)."
      ],
      "metadata": {
        "id": "n7k3mCLl9Qt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[4]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'Score': [85, 90, 95]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nDescription:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "_Pj_78XW9XvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .shape attribute tells you the dimensions of the DataFrame, like how many rows and columns it has. For example, it might say (3, 3), meaning 3 rows and 3 columns. The .describe() method gives you a summary of the numerical columns, like the count, mean, standard deviation, minimum, maximum, and quartiles. It’s useful to quickly understand your data’s stats. In my example, I made a small DataFrame with names, ages, and scores. The shape will show the size, and describe() will show stats for Age and Score since Name is text. I printed both to see what they do."
      ],
      "metadata": {
        "id": "meFa4OBS9h49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[5]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('airtravel.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "wzZ3kgqj9mju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used pandas to load the CSV file called ‘airtravel.csv’ into a DataFrame. The head() function prints the first 5 rows by default, which is a quick way to peek at the data. Without seeing the actual file, I’d guess it has columns like years and months, maybe with passenger numbers. If it’s air travel data, I might notice things like some years or months having more passengers, or maybe some missing values, or that the data is organized in a table format with numerical and categorical data. I’d need to check if the columns make sense, like if there’s a “Year” and “Passengers” column, and whether the numbers look reasonable or if there are any obvious errors."
      ],
      "metadata": {
        "id": "E1AIp49y9rPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[6]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('airtravel.csv')\n",
        "\n",
        "# Find month with highest total passengers\n",
        "total_passengers = df.sum()\n",
        "highest_month = total_passengers.idxmax()\n",
        "highest_value = total_passengers.max()\n",
        "\n",
        "# Find lowest in 1958 (assuming 1958 is a column)\n",
        "if '1958' in df.columns:\n",
        "    lowest_1958 = df['1958'].min()\n",
        "    lowest_month_1958 = df['1958'].idxmin()\n",
        "else:\n",
        "    print(\"1958 column not found\")\n",
        "\n",
        "print(\"Month with highest total passengers:\", highest_month, \"with\", highest_value, \"passengers\")\n",
        "if '1958' in df.columns:\n",
        "    print(\"Month with lowest passengers in 1958:\", lowest_month_1958, \"with\", lowest_1958, \"passengers\")"
      ],
      "metadata": {
        "id": "Y1we7n3K9vy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I loaded the same ‘airtravel.csv’ file again using pandas. To find the month with the highest total passengers, I summed all the values in the DataFrame (assuming rows are months and columns are years) and used idxmax() to get the month name and max() for the value. For the lowest in 1958, I checked if ‘1958’ is a column (if the data has years as columns) and found the minimum value and its index. If the column wasn’t there, I printed a message. I think the data might have months as rows and years as columns, so the highest total would be the row (month) with the most passengers across all years, and for 1958, it’s just looking at that year’s data. The output should show the month names and passenger numbers, but I’m not sure if the CSV structure is exactly like this."
      ],
      "metadata": {
        "id": "yVSSxmC2-uiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[7]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Bob'], 'Score': [85, 90, 95, 88, 92], 'City': ['NY', 'LA', 'NY', 'LA', 'NY']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "grouped = df.groupby('Name')\n",
        "\n",
        "for name, group in grouped:\n",
        "    print(name)\n",
        "    print(group)\n",
        "    print()\n",
        "\n",
        "average_scores = grouped['Score'].mean()\n",
        "print(\"Average scores by name:\")\n",
        "print(average_scores)"
      ],
      "metadata": {
        "id": "YRG7C5on-0KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .groupby() function in pandas is used to group data based on a column’s values, so you can do things like calculate averages or sums for each group. In my example, I made a DataFrame with names, scores, and cities, and I grouped it by ‘Name’. This means it splits the data into groups where each group has all the rows for a specific name, like all rows for “Alice” together. Then I printed each group and calculated the average score for each name. You can see how it organizes the data, which is useful for summarizing or analyzing things like sales by region or scores by student."
      ],
      "metadata": {
        "id": "FiwPMxue-3ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[8]\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "missing_values = titanic.isnull().sum()\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "uelZJfNX_ErH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used seaborn to load the Titanic dataset, which has info about passengers like age, sex, and whether they survived. To count missing values, I used isnull() to check for NaN (not a number) in each cell, then summed them up with sum() for each column. The output shows how many missing values are in each column, like “age” or “embarked.” I think there might be a lot of missing ages because some passenger records might not have that info, or maybe “cabin” has a lot missing because not everyone had a cabin recorded."
      ],
      "metadata": {
        "id": "xY24WyeZ_IuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[9]\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "titanic = sns.load_dataset('titanic')\n",
        "titanic['Age'].hist(bins=20)\n",
        "plt.title(\"Age Distribution\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QoN-va0P_N8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I plotted a histogram of the ‘Age’ column from the Titanic dataset to see how ages are distributed. The hist() function makes bars showing how many people fall into each age range. I think the shape might be skewed, maybe to the right, meaning there are more younger people and fewer older ones. This could happen because the Titanic had a lot of younger passengers, like families or workers, and fewer elderly people. Another reason might be that age data is missing for some older people, or the passenger list just had more younger folks. The plot should show if there’s a peak somewhere and if the tail stretches out more on one side."
      ],
      "metadata": {
        "id": "5aZDHoVP_RX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[10]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "low_var = [10, 11, 12, 13, 14]  # Should have low std dev\n",
        "high_var = [5, 15, 25, 35, 45]  # Should have high std dev\n",
        "\n",
        "df = pd.DataFrame({'Low_Var': low_var, 'High_Var': high_var})\n",
        "\n",
        "low_std = df['Low_Var'].std()\n",
        "high_std = df['High_Var'].std()\n",
        "\n",
        "print(\"Standard deviation of low variance:\", low_std)\n",
        "print(\"Standard deviation of high variance:\", high_std)"
      ],
      "metadata": {
        "id": "rCNVjizP_WWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high standard deviation means the values in a column are spread out a lot from the average, so there’s a lot of variability. A low standard deviation means the values are close to the average, so they don’t vary much. In my example, I made two lists: one with numbers close together (10 to 14), which should have a low standard deviation, and another with numbers far apart (5 to 45), which should have a high one. I put them in a DataFrame and used std() to calculate the standard deviation for each. The low variance column should have a small number for std, and the high variance one should have a bigger number, showing how spread out the data is."
      ],
      "metadata": {
        "id": "urqHzbhc_bST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[11]\n",
        "\n",
        "Missing data can mess up analyses because it can skew results or make them incomplete. For example, in medical research, if patient age or test results are missing, you might not get an accurate picture of how a disease affects different age groups, which could lead to wrong conclusions about treatments. Another example is in sales data: if some stores don’t report their sales for certain months, you might think sales are lower than they really are, or you might miss trends, like which products are selling best. Missing data can bias your findings or make models less reliable because they’re not using all the info they should."
      ],
      "metadata": {
        "id": "6K3bNMgl_nye"
      }
    }
  ]
}